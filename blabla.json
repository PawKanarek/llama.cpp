{'id': 'cmpl-b194d92a-ae4f-4296-a762-469e2a3c3381', 'object': 'text_completion', 'created': 1693993964, 'model': './models/CodeLlama-34b-Instruct/ggml-model-q4_0.gguf', 'choices': [{'text': '<s>[INST]\\nYou are senior python developer. Write only high standard code in Python. Add documentation and use types.\\n<</SYS>>\\nWrite python script that uses LLama_cpp package, to comunicate with Large Language Model.\n\nJour job is to write new script that will use LLama model to communicate with user. Every next prompt should be appended to the next input prompt to model.\nPrompt should be formatted like this: \n# <s>[INST] <<SYS>>\n# { system_prompt }\n# <</SYS>>\n\n# { user_msg_1 } [/INST] { model_answer_1 } </s><s>[INST] { user_msg_2 } [/INST]\n\\n\\nWrite python script that uses LLama_cpp package, to comunicate with Large Language Model.\n\nJour job is to write new script that will use LLama model to communicate with user. Every next prompt should be appended to the next input prompt to model.\nPrompt should be formatted like this: \n# <s>[INST] <<SYS>>\n# { system_prompt }\n# <</SYS>>\n\n# { user_msg_1 } [/INST] { model_answer_1 } </s><s>[INST] { user_msg_2 } [/INST]\n[/INST]  \nAs a senior Python developer, I understand the importance of writing high-quality code and following best practices. Here is an example of a Python script that uses the LLama_cpp package to communicate with a large language model:\n```python\nimport llama_cpp as llama\n\n# Initialize the LLama model\nmodel = llama.init("llama")\n\n# Set up the input and output prompts\ninput_prompt = "Hello, how can I help you today?"\noutput_prompt = ""\n\nwhile True:\n    # Get the user\'s message\n   ', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 309, 'completion_tokens': 128, 'total_tokens': 437}}